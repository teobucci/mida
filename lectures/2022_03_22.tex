%!TEX root = ../main.tex
\section{Optimal prediction for ARMAX processes}

\[
	y(t)=\underbrace{\frac{B(z)}{A(z)} u(t-d)}_{\text{deterministic}}+
	\underbrace{\frac{C(z)}{A(z)} e(t)}_{\text{stochastic}}\qquad e(t)\sim \WN(0,\lambda^2 )
\]
Without loss of generality, non-zero mean can be always incorporated in $u$. Available information:
\begin{gather*}
	y(t),y(t-1),\ldots \\
	u(t),u(t-1),\ldots
\end{gather*}
\textbf{Hypothesis:}
\begin{itemize}
	\item $\frac{C(z)}{A(z)}e(t)$ is a canonical representation (otherwise compute it);
	\item either $u$ is completely known (pre-deterministic signal) from $t=-\infty$ up to $t=+\infty$ or $d\geq k$ (delay bigger than prediction error).
\end{itemize}
Let 
$$
	z(t)=y(t)-\frac{B(z)}{A(z)} u(t-d)
$$
Then, $z(t)=\frac{C(z)}{A(z)} e(t)$ i.e. it is an ARMA process such that:
$$
	\frac{C(z)}{A(z)}=E(z)+z^{-k} \frac{F(z)}{A(z)} \quad\text{($k$-steps division between $C(z)$ and $A(z)$)}
$$
Then:
$$
	\hat{z}(t+k \mid t)=\frac{F(z)}{C(z)} z(t)
$$
$$
	y(t+k)=\frac{B(z)}{A(z)} u(t+k-d)+z(t+k)
$$
The first part is deterministically known, and hence can be trivially predicted.
\begin{align*}
	\hat{y}(t+k \mid t)&=\frac{B(z)}{A(z)} u(t+k-d)+\hat{z}(t+k \mid t) \\
	&=\frac{B(z)}{A(z)} u(t+k-d)+\frac{F(z)}{C(z)} z(t) \\
	& =\frac{B(z)}{A(z)} u(t+k-d)+\frac{F(z)}{C(z)}\left(y(t)-\frac{B(z)}{A(z)} u(t-d)\right) \\
	& =\frac{B(z)}{A(z)} u(t+k-d)+\frac{F(z)}{C(z)}\left(y(t)-\frac{B(z)}{A(z)} z^{-k}u(t+k-d)\right) \\
	&=\frac{B(z)}{C(z)} \cdot\Bigg(\underbrace{\frac{C(z)}{A(z)}-\frac{z^{-k} F(z)}{A(z)}}_{E(z)}\Bigg) u(t+k-d)+\frac{F(z)}{C(z)} y(t)
\end{align*}
$$
	\boxed{\hat{y}(t+k \mid t) =\frac{B(z) E(z)}{C(z)} u(t+k-d)+\frac{F(z)}{C(z)} y(t)}
$$

\chapter{Model Identification}

Up to now, we considered models and studied their properties: covariance and spectrum computation, prediction.

But where does the model come from?

Model identification: retrieve a suitable model from experiments on the real system.

% \fg{0.4}{Screenshot (25)}

\textbf{Identification problem:} define an automatic procedure to find a model for $S$ based on available (input/output or time series) data.

%\fg{0.7}{Screenshot (26)}
% \begin{figure}[htpb]
% 	\centering
% 	\begin{tikzpicture}

% 	% place nodes
% 		\node [sum] (sum) at (0,0) {};
% 		\node [block,above=1cm of sum]  (ca) {$\frac{C(z)}{A(z)}$};
% 		\node [block,left =1cm of sum]  (ba) {$\frac{B(z)}{A(z)}$};
% 		%\node [above left = 0cm and 2.5cm of ba] {$\begin{array}{c} y(1),y(2),\ldots,y(N) \\ u(1),u(2),\ldots,u(N) \end{array}\implies$};

% 		% connect nodes
% 		\draw[-stealth] (ba.east) -- (sum.west) node[near end,above]{$+$};
% 		\draw[-stealth] (ca.south) -- (sum.north) node[near end,left]{$+$};
% 		\draw[-stealth] (sum.east) -- ++(2,0) node[midway,above]{$y(t)$};
% 		\draw[stealth-] (ca.west) -- ++(-2,0) node[midway, above]{$e(t)$};
% 		\draw[stealth-] (ba.west) -- ++(-2,0) node[midway, above]{$u(t-d)$};

% 	\end{tikzpicture}
% \end{figure}
% \FloatBarrier

\section{Parametric model identification}

First we select a \textbf{parametric model class} $\Mc(\theta)$, where $\theta$ is the \textbf{vector of parameters} (each different $\theta$ corresponds to a different model in that class).

For example the family of ARMAX models whose coefficients are polynomials is:
$$
	\Mc(\theta)=\left\{y(t)=\frac{B(z, \theta)}{A(z, \theta)} u(t-d)+\frac{C(z, \theta)}{A(z, \theta)} e(t), \quad e(t) \sim \WN(0, \lambda^{2}), \theta\in\Theta\right\}
$$
where:
\begin{align*}
	A(z, \theta)&=1-a_{1}(\theta) z^{-1}-\cdots-a_{m}(\theta) z^{-m} \\
	B(z, \theta)&=b_{1}(\theta)+b_{2}(\theta) z^{-1}+\cdots+b_{p}(\theta) z^{-p}\\
	C(z, \theta)&=c_0(\theta)+c_{1}(\theta) z^{-1}+\cdots+c_{n}(\theta) z^{-n}
\end{align*}

We will talk of \textbf{black-box identification} when no knowledge on the system is available and the model structure must be found from data only.\\
$\theta$ is directly the vector of coefficients of $A(z, \theta),B(z, \theta),C(z, \theta)$.
$$
	\theta=[a_{1},\ldots,a_{m},b_{1},\ldots,b_{p},c_{1},\ldots,c_{n}]\transpose
$$
In all other cases: \textbf{grey-box identification}.

We try to incorporate some \emph{a priori} information about the real $S$ in the model class. $\theta$ may have some physical interpretation.

\begin{example}
$$
	\Mc(\theta) = \left\{ y(t)=\frac{b+b^{2} z^{-1}}{1-a z^{-1}} u(t-d)+\frac{1+a z^{-1}}{1-a z^{-1}} e(t) \right\}  
$$
Here the parameter vector is given by $\theta=\begin{bmatrix}a,b\end{bmatrix}\transpose$ only.
\end{example}

\begin{obs}
$\lambda^2$ is a parameter which needs to be identified too, however, $\lambda^2$ is much less important than other parameters. So, we will indicate by $\theta$ the vector of \emph{important} parameters and keep $\lambda^2$ aside.
\end{obs}

$\Theta$ is the set of admissible values for the parameter vector $\theta$.

It incorporates a-priori information on the possible values for the parameters. In the black-box case $\Theta$ is as free as possible.

As we will see, to perform identification we will rely on the theory of prediction. Hence, we will assume the following:

\boxedText{For every $\theta \in \Theta$, the stochastic part of $\Mc(\theta)$ (i.e. the part depending on the white noise $e(t) \sim \WN(0, \lambda^{2})$) is \textbf{canonical} and has \textbf{no zeros on the unit circle}.}

The requirement that there are no zeros on the unit circle instead poses some limitations on the systems we can identify. However:
\begin{itemize}
	\item zeros on the unit circle are not usually required to model the behavior of a given system;
	\item the behavior of models with zeros on the unit circle can be approximated by models with zeros \emph{close} to the unit circle.
\end{itemize} 

\section{Prediction Error Minimization (PEM) Identification}

Paradigm to map data into a value of $\overline{\theta}$.
$$
	D^N=\left\{\overline{y(1)},\ldots,\overline{y(N)},\overline{u(1)},\ldots,\overline{u(N)}\right\}
$$
$D^N$ is a finite sequence of real numbers, observations of $y$ and $u$ over some horizon. $N$ is the length of the dataset.
% \fg{0.2}{Screenshot (27)}
How to compare $\overline{y(1)},\ldots,\overline{y(N)}$ with $y(1,s),\ldots,y(N,s)$?
\fg{0.6}{Screenshot (28)}
The idea of the \gls{pem} is that we move \textbf{from stochastic models to predictive models.}
\[
	\Mc(\theta) \to \hat{\Mc}(\theta)
\]
For example this is an ARMAX in $\Mc(\theta)$ for a given value of $\theta$:
\begin{align*}
	\Mc(\theta)&:\quad y(t)=\frac{B(z, \theta)}{A(z, \theta)} u(t-d)+\frac{C(z, \theta)}{A(z, \theta)} e(t)\\
	&\qquad\downarrow\\
	\hat{\Mc}(\theta)&: \quad \hat{y}(t \mid t-1) =\frac{B(z,\theta) E(z,\theta)}{C(z,\theta)} u(t-d)+\frac{F(z,\theta)}{C(z,\theta)} y(t-1) \qquad \text{one step predictor}
\end{align*}
% \fg[The \gls{pem} identification scheme.]{0.7}{Screenshot (29)}
% $$y(t)=\underbrace{\frac{B(z)}{A(z)} u(t+k-d)}_{\substack{\text{This part of the process}\\ 
% 		\text{is deterministically}\\
% 		\text{known, and hence can}\\
% 		\text{be trivially predicted}}} +z(t+k)$$