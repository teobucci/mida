%!TEX root = ../main.tex

\section{Identification of ARMA/ARMAX models (Maximum Likelihood (ML) method)}

We consider a generic ARMAX:
\[
	M(\theta ): \quad y(t)=\frac{B(z,\theta )}{A(z,\theta)} u(t-d)+\frac{C(z,\theta )}{A(z,\theta)}e(t)\quad e(t)\sim \WN(0,\lambda^2)
\]
where
\begin{align*}
	A(z)&=1-a_{1} z^{-1}-a_{2} z^{-2}-\cdots-a_{m} z^{-m} \\
	B(z)&=b_{0}+b_{1} z^{-1}+b_{2} z^{-2}+\cdots+b_{p} z^{-p} \\
	C(z)&=1+c_{1} z^{-1}+c_{2} z^{-2}+\cdots+c_{n} z^{-n}
\end{align*}
We assume that $C(z,\theta)\neq 1$. Since $A$ and $C$ are monic the first term of the long division always gives $1$ and the remainder is always $C-A$. The predictor is then:
\[
	\hat{M}(\theta): \quad \hat{y}(t \mid t-1, \theta)=\frac{C(z,\theta)-A(z,\theta)}{C(z,\theta)} y(t)+\frac{B(z,\theta)}{C(z,\theta)} u(t-d)
\]
And the prediction error is:
\begin{align*}
	\varepsilon(t, \theta)=y(t)-\hat{y}(t \mid t-1, \theta)&=\left[1-\frac{C(z,\theta)-A(z,\theta)}{C(z,\theta)}\right] y(t)-\frac{B(z,\theta)}{C(z,\theta)} u(t-d)\\
	&=\frac{A(z,\theta)}{C(z,\theta)} y(t)-\frac{B(z,\theta)}{C(z,\theta)} u(t-d)
\end{align*}
Since we have $C(z,\theta)$ in the denominator, the error is \emph{no more linear} in $\theta$.
The cost function is \emph{non-convex} and may present \emph{local} minima:
\[
	J_{N}(\theta)=\frac{1}{N} \sum_{t=1}^{N} \varepsilon(t, \theta)^{2}
\]
To tackle the non-linearity we can use some kinds of numerical optimization using descent methods:
\begin{itemize}
	\item the algorithm is initialized with an initial estimate (typically randomly chosen) of the optimal parameter vector: $\theta^{0}$;
	\item update rule: $\theta^{i+1} = f (\theta^{i})$;
	\item the sequence of estimates should converge to $\hat\theta_{N}$.
\end{itemize}
If there are local minima, we can just use an \emph{empirical} approach by running the algorithm with many different starting values getting a range of candidates of minimum values, hoping to explore the entire domain and not miss the global minimum. This of course comes with the cost of computational complexity.


\section{Asymptotic Analysis of PEM Identification}

Is $M(\hat\theta_{N})$ a good model for the process $y(t)$?
We can give an asymptotic answer as $N\to \infty$

\textbf{Assumption on the data generating system:}

$y(t),u(t)$ are \gls{ssp} generated by a \textbf{linear system:}\footnote{Not necessarily of the same type as in $M$.}
\[
	S:
	\begin{cases}
		y(t) = G(z)u(t) + H(z)e(t)\\
		u(t) = F(z)r(t) + S(z)e(t)
	\end{cases}
	\qquad
	\begin{array}{l}
		e(t)\sim \WN(0,\lambda^2)\\
		r(t)\sim \WN(0,\sigma^2)
	\end{array}
\]
where $G(z),H(z),F(z),S(z)$ are \textbf{asymptotically stable, rational} transfer functions.

The most typical cases are when:
\begin{itemize}
	\item $S(z)=0$ (\textbf{open-loop experiment})
	\fg{0.6}{Screen Shot 2022-04-02 at 10.44.03}
	\item $S(z)\neq 0$ (\textbf{closed-loop experiment}), $S(z)$ accounts for $u(t)$ depending on $e(t)$ because of the feedback.
	\fg{0.6}{Screen Shot 2022-04-02 at 10.45.22}
\end{itemize}
The measured data sequence corresponds to a particular \textbf{realization} of input/output signals of $S$:
\[
	D^{N}=
	\begin{cases}
	 	u(1),u(2),\ldots,u(N)\\
	 	y(1),y(2),\ldots,y(N)
	\end{cases}
	\quad
	\text{to be thought as}
	\quad
	\begin{cases}
	 	u(1,\overline{s}),u(2,\overline{s}),\ldots,u(N,\overline{s})\\
	 	y(1,\overline{s}),y(2,\overline{s}),\ldots,y(N,\overline{s})
	\end{cases}
\]
Hence also the predictor computed from the given realization should be thought as:
\[
	\hat{y}(i\mid i-1,\theta) = f(D^{N}) = \hat{y}(i\mid i-1,\theta,\overline{s})
\]
and also:
\begin{gather*}
	\varepsilon(i,\theta) = y(i,\overline{s}) - \hat{y}(i\mid i-1,\theta,\overline{s}) = \varepsilon(i,\theta,\overline{s})
	\quad
	J_{N}(\theta) = \frac{1}{N}\sum_{i=1}^{N} \varepsilon(i,\theta,\overline{s})^2 = J_{N}(\theta,\overline{s})\\
	\hat{\theta}_{N} = \argmin_{\theta} J_{N}(\theta,\overline{s}) = \hat{\theta}_{N}(\overline{s})
\end{gather*}
\fg{0.6}{Screen Shot 2022-04-02 at 11.01.06}
Depending on the experiment, different costs and minimizers. As $N\to \infty$, the sequence of minimizers \textbf{converges:}\footnote{In practice, a good number could be $N\geq 300$.}
\fg{0.6}{Screen Shot 2022-04-02 at 11.04.00}
Indeed we have the following result.
\begin{theorem}
	Under the current assumptions, as the number of data points becomes larger and larger, we have with probability one that:
	\[
		J_{N}(\theta,s) = \frac{1}{N}\sum_{i=1}^{N} \varepsilon(i,\theta,s)^2 \xrightarrow{N\to\infty} \E[\varepsilon(t,\theta,s)^2] = \overline{J}(\theta)
	\]
	The convergence is almost sure w.r.t. $s$, uniform w.r.t. $\theta$ (the error goes to $0$ with the same rate for all $\theta$).\\
	Moreover if we define the \textbf{set of all minimizers:}
	\[
		\Delta = \left\{ \theta ^{\star} : \overline{J}(\theta ^{\star})\leq \overline{J}(\theta),\forall \theta  \right\}
	\]
	we have:
	\[
		\text{distance}[\hat{\theta}_{N}(s), \Delta]\xrightarrow{N\to\infty} 0
	\]
\end{theorem}
As a corollary we have that if $\Delta ={\theta ^{\star} }$ (i.e. $J_{N}(\theta)$ has a unique minimum point), then:
\[
	\hat{\theta}_{N}(s) \xrightarrow{N\to\infty} \theta ^{\star} 
\]
almost surely and regardless of the experiment.

To sum up, as $N$ is large enough we can approximate $M(\hat{\theta}(s)\approx M(\theta ^{\star}$.

Question: is $M(\theta^{\star}$ satisfactory? Let's assume that the systeam we are dealing with belongs to the class in which we construct the model, $S\in \mathcal{M}$. This is an ideal situation, we have enough degrees of freedom to describe the mechanism by which $y(t)$ is generated. That means that $\exists\theta^{0}: M(\theta^{0}) = S$.